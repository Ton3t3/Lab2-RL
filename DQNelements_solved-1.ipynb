{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f9a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright [2025] [KTH Royal Institute of Technology] \n",
    "# Licensed under the Educational Community License, Version 2.0 (ECL-2.0)\n",
    "# This file is part of the materials for EL2805 - Reinforcement Learning - Exercise Session 3 at KTH, Stockholm.\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/250: Total Reward: 18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1737968801.py:143: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  states = torch.tensor(states, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2/250: Total Reward: 26.0\n",
      "Episode 3/250: Total Reward: 21.0\n",
      "Episode 4/250: Total Reward: 18.0\n",
      "Episode 5/250: Total Reward: 17.0\n",
      "Episode 6/250: Total Reward: 14.0\n",
      "Episode 7/250: Total Reward: 21.0\n",
      "Episode 8/250: Total Reward: 49.0\n",
      "Episode 9/250: Total Reward: 20.0\n",
      "Episode 10/250: Total Reward: 39.0\n",
      "Episode 11/250: Total Reward: 26.0\n",
      "Episode 12/250: Total Reward: 20.0\n",
      "Episode 13/250: Total Reward: 12.0\n",
      "Episode 14/250: Total Reward: 9.0\n",
      "Episode 15/250: Total Reward: 39.0\n",
      "Episode 16/250: Total Reward: 13.0\n",
      "Episode 17/250: Total Reward: 12.0\n",
      "Episode 18/250: Total Reward: 15.0\n",
      "Episode 19/250: Total Reward: 17.0\n",
      "Episode 20/250: Total Reward: 16.0\n",
      "Episode 21/250: Total Reward: 17.0\n",
      "Episode 22/250: Total Reward: 12.0\n",
      "Episode 23/250: Total Reward: 16.0\n",
      "Episode 24/250: Total Reward: 25.0\n",
      "Episode 25/250: Total Reward: 55.0\n",
      "Episode 26/250: Total Reward: 37.0\n",
      "Episode 27/250: Total Reward: 13.0\n",
      "Episode 28/250: Total Reward: 26.0\n",
      "Episode 29/250: Total Reward: 12.0\n",
      "Episode 30/250: Total Reward: 28.0\n",
      "Episode 31/250: Total Reward: 37.0\n",
      "Episode 32/250: Total Reward: 14.0\n",
      "Episode 33/250: Total Reward: 27.0\n",
      "Episode 34/250: Total Reward: 12.0\n",
      "Episode 35/250: Total Reward: 28.0\n",
      "Episode 36/250: Total Reward: 31.0\n",
      "Episode 37/250: Total Reward: 37.0\n",
      "Episode 38/250: Total Reward: 22.0\n",
      "Episode 39/250: Total Reward: 13.0\n",
      "Episode 40/250: Total Reward: 47.0\n",
      "Episode 41/250: Total Reward: 23.0\n",
      "Episode 42/250: Total Reward: 27.0\n",
      "Episode 43/250: Total Reward: 28.0\n",
      "Episode 44/250: Total Reward: 47.0\n",
      "Episode 45/250: Total Reward: 24.0\n",
      "Episode 46/250: Total Reward: 51.0\n",
      "Episode 47/250: Total Reward: 15.0\n",
      "Episode 48/250: Total Reward: 22.0\n",
      "Episode 49/250: Total Reward: 52.0\n",
      "Episode 50/250: Total Reward: 25.0\n",
      "Episode 51/250: Total Reward: 18.0\n",
      "Episode 52/250: Total Reward: 28.0\n",
      "Episode 53/250: Total Reward: 26.0\n",
      "Episode 54/250: Total Reward: 14.0\n",
      "Episode 55/250: Total Reward: 45.0\n",
      "Episode 56/250: Total Reward: 53.0\n",
      "Episode 57/250: Total Reward: 23.0\n",
      "Episode 58/250: Total Reward: 42.0\n",
      "Episode 59/250: Total Reward: 65.0\n",
      "Episode 60/250: Total Reward: 15.0\n",
      "Episode 61/250: Total Reward: 60.0\n",
      "Episode 62/250: Total Reward: 40.0\n",
      "Episode 63/250: Total Reward: 45.0\n",
      "Episode 64/250: Total Reward: 37.0\n",
      "Episode 65/250: Total Reward: 43.0\n",
      "Episode 66/250: Total Reward: 22.0\n",
      "Episode 67/250: Total Reward: 59.0\n",
      "Episode 68/250: Total Reward: 17.0\n",
      "Episode 69/250: Total Reward: 73.0\n",
      "Episode 70/250: Total Reward: 22.0\n",
      "Episode 71/250: Total Reward: 200.0\n",
      "Evaluation Total Reward: 74.0\n"
     ]
    }
   ],
   "source": [
    "# Copyright [2025] [KTH Royal Institute of Technology] \n",
    "# Licensed under the Educational Community License, Version 2.0 (ECL-2.0)\n",
    "# This file is part of the materials for EL2805 - Reinforcement Learning - Exercise Session 3 at KTH, Stockholm.\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\"\"\"Tensorboard instructions:\n",
    "    1. Install library using conda install tensorboard\n",
    "    2. After running the script, open the command line (in anaconda) \n",
    "    3. Run tensorboard --logdir=runs/cartpole_dqn/\n",
    "    4. Copy the address that you get - probably http://localhost:6006/\n",
    "    5. You should be able to see logged losses, rewards and epsilon values. \n",
    "    6. If you want to log in additional quantities use writer.add_scalar() as below\"\"\"\n",
    "    \n",
    "writer = SummaryWriter(log_dir=\"runs/cartpole_dqn\")\n",
    "\n",
    "\n",
    "# Define Experience tuple\n",
    "# Experience represents a transition in the environment, including the current state, action taken,\n",
    "# received reward, next state, and whether the episode is done.\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ExperienceReplayBuffer:\n",
    "    \"\"\"Replay buffer for storing experiences.\n",
    "    \n",
    "       The experience replay buffer stores past experiences so that the agent can learn from them later.\n",
    "       By sampling randomly from these experiences, the agent avoids overfitting to the most recent \n",
    "       transitions and helps stabilize training.\n",
    "       - The buffer size is limited, and older experiences are discarded to make room for new ones.\n",
    "       - Experiences are stored as tuples of (state, action, reward, next_state, done).\n",
    "       - A batch of experiences is sampled randomly during each training step for updating the Q-values.\"\"\"\n",
    "\n",
    "    def __init__(self, maximum_length):\n",
    "        self.buffer = deque(maxlen=maximum_length)  # Using deque ensures efficient removal of oldest elements\n",
    "\n",
    "    def append(self, experience):\n",
    "        \"\"\"Add a new experience to the buffer\"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of the buffer\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample_batch(self, n):\n",
    "        \"\"\"Randomly sample a batch of experiences\"\"\"\n",
    "        if n > len(self.buffer):\n",
    "            raise IndexError('Sample size exceeds buffer size!')\n",
    "        indices = np.random.choice(len(self.buffer), size=n, replace=False)  # Random sampling\n",
    "        batch = [self.buffer[i] for i in indices]  # Create a batch from sampled indices\n",
    "        return zip(*batch)  # Unzip batch into state, action, reward, next_state, and done\n",
    "\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    \"\"\"Feedforward neural network that approximates the Q-function.\n",
    "    \n",
    "       The network takes the current state as input and outputs Q-values for all possible actions.\n",
    "       The action corresponding to the highest Q-value is considered the optimal action.\n",
    "       - The input size corresponds to the state dimension of the environment.\n",
    "       - The network has one hidden layer with 64 neurons and ReLU activation.\n",
    "       - The output layer has one neuron per action (Q-values for each action).\"\"\"\n",
    "        \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_size, 64)  # First layer: state -> hidden layer\n",
    "        self.hidden_layer = nn.Linear(64, 64)  # Second layer: hidden -> hidden layer\n",
    "        self.output_layer = nn.Linear(64, output_size)  # Output layer: hidden -> Q-values\n",
    "        self.activation = nn.ReLU()  # ReLU activation function for hidden layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Define forward pass\"\"\"\n",
    "        x = self.activation(self.input_layer(x))  # Apply input layer and ReLU\n",
    "        x = self.activation(self.hidden_layer(x))  # Apply hidden layer and ReLU\n",
    "        return self.output_layer(x)  # Return Q-values for all actions\n",
    "\n",
    "\n",
    "### Parameters ###\n",
    "GAMMA = 0.99  # Discount factor (how much future rewards are considered)\n",
    "EPSILON = 1.0  # Initial exploration rate (balance between exploration and exploitation)\n",
    "EPSILON_MIN = 0.01  # Minimum exploration rate\n",
    "EPSILON_DECAY = 0.995  # Decay rate for epsilon after each episode\n",
    "BATCH_SIZE = 32  # Number of experiences to sample from the replay buffer per update\n",
    "BUFFER_SIZE = 10000  # Size of the replay buffer\n",
    "LEARNING_RATE = 0.001  # Learning rate for the optimizer\n",
    "N_EPISODES = 250  # Number of training episodes\n",
    "MAX_STEPS = 200  # Maximum number of steps per episode\n",
    "\n",
    "# Initialize environment, buffer, network, and optimizer\n",
    "env = gym.make('CartPole-v1')  # Create the CartPole environment\n",
    "\n",
    "# Initialize experience replay buffer\n",
    "buffer = ExperienceReplayBuffer(maximum_length=BUFFER_SIZE)\n",
    "\n",
    "# Initialize the Q-network (state -> Q-values for actions)\n",
    "network = MyNetwork(input_size=env.observation_space.shape[0], output_size=env.action_space.n)\n",
    "\n",
    "# Optimizer for training the Q-network\n",
    "optimizer = optim.Adam(network.parameters(), lr=LEARNING_RATE)  # Adam optimizer for efficient training\n",
    "\n",
    "\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    \"\"\"Epsilon-greedy action selection\n",
    "    # We balance exploration and exploitation using epsilon-greedy.\n",
    "    # Exploration: Choose a random action.\n",
    "    # Exploitation: Choose the action with the highest Q-value (the optimal action).\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()  # Explore by selecting a random action\n",
    "    else:\n",
    "        state_tensor = torch.tensor([state], dtype=torch.float32)  # Convert state to tensor\n",
    "        return network(state_tensor).argmax().item()  # Exploit by selecting the action with max Q-value\n",
    "\n",
    "# Training loop\n",
    "for episode in range(N_EPISODES):\n",
    "    state = env.reset()[0]  # Reset environment and get initial state\n",
    "    total_reward = 0\n",
    "    for t in range(MAX_STEPS):\n",
    "        # Choose action using epsilon-greedy policy\n",
    "        action = select_action(state, EPSILON)\n",
    "\n",
    "        # Execute action in environment and get feedback (next state, reward, etc.)\n",
    "        next_state, reward, terminal, truncated, _ = env.step(action)\n",
    "        done = terminal or truncated  # Done is True if episode ends\n",
    "        total_reward += reward\n",
    "\n",
    "        # Store the experience (state, action, reward, next state, done) in the buffer\n",
    "        buffer.append(Experience(state, action, reward, next_state, done))\n",
    "        state = next_state  # Update state for the next step\n",
    "\n",
    "        # Training step: update Q-values using a batch of experiences from the buffer\n",
    "        if len(buffer) >= BATCH_SIZE:\n",
    "            # Sample a batch of experiences from the buffer\n",
    "            states, actions, rewards, next_states, dones = buffer.sample_batch(BATCH_SIZE)\n",
    "\n",
    "            # Convert the batch data into tensors\n",
    "            states = torch.tensor(states, dtype=torch.float32)\n",
    "            actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)  # Unsqueeze for correct shape\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "            # Compute Q-values for the current states\n",
    "            q_values = network(states).gather(1, actions).squeeze()  # Q-values for taken actions\n",
    "\n",
    "            # Compute the target Q-values for the next states\n",
    "            with torch.no_grad():  # No need to compute gradients for target Q-values\n",
    "                next_q_values = network(next_states).max(1)[0]  # Max Q-value for next state\n",
    "                targets = rewards + GAMMA * next_q_values * (1 - dones)  # Target: Bellman equation\n",
    "\n",
    "            # Compute the loss (MSE loss between predicted Q-values and target Q-values)\n",
    "            loss = nn.functional.mse_loss(q_values, targets)\n",
    "               \n",
    "            # Backpropagation step: update network parameters\n",
    "            optimizer.zero_grad()  # Zero gradients before backpropagation\n",
    "            loss.backward()  # Compute gradients\n",
    "            nn.utils.clip_grad_norm_(network.parameters(), max_norm=1.0)  # Clip gradients to avoid exploding gradients\n",
    "            optimizer.step()  # Update parameters\n",
    "\n",
    "            # Log loss to TensorBoard for visualization\n",
    "            writer.add_scalar(\"Loss\", loss.item(), episode * MAX_STEPS + t)\n",
    "\n",
    "        if done:  # If the episode ends\n",
    "            break\n",
    "\n",
    "    # Decay epsilon: reduce exploration over time\n",
    "    EPSILON = max(EPSILON_MIN, EPSILON * EPSILON_DECAY)\n",
    "    \n",
    "    # Log total reward and epsilon to TensorBoard\n",
    "    writer.add_scalar(\"Total Reward\", total_reward, episode)\n",
    "    writer.add_scalar(\"Epsilon\", EPSILON, episode)\n",
    "\n",
    "    # Print progress for each episode\n",
    "    print(f\"Episode {episode + 1}/{N_EPISODES}: Total Reward: {total_reward}\")\n",
    "    if total_reward >= 200:  # If the agent achieves good performance, stop early\n",
    "        break\n",
    "\n",
    "# Close the environment after training\n",
    "env.close()\n",
    "\n",
    "\n",
    "# Evaluate the trained policy by rendering it\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "state = env.reset()[0]\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "# Run the trained agent in the environment\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = select_action(state, 0)  # Choose action (epsilon=0, i.e., exploit the policy)\n",
    "    next_state, reward, done, truncated, _ = env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Evaluation Total Reward: {total_reward}\")\n",
    "env.close()\n",
    "\n",
    "writer.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

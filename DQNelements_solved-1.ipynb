{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f1f9a579",
      "metadata": {
        "id": "f1f9a579"
      },
      "outputs": [],
      "source": [
        "# Copyright [2025] [KTH Royal Institute of Technology]\n",
        "# Licensed under the Educational Community License, Version 2.0 (ECL-2.0)\n",
        "# This file is part of the materials for EL2805 - Reinforcement Learning - Exercise Session 3 at KTH, Stockholm.\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import deque, namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HISBWdKAJDME",
        "outputId": "8cce9345-91f1-4d85-8a74-6c624b697bc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/250: Total Reward: 12.0\n",
            "Episode 2/250: Total Reward: 27.0\n",
            "Episode 3/250: Total Reward: 28.0\n",
            "Episode 4/250: Total Reward: 14.0\n",
            "Episode 5/250: Total Reward: 43.0\n",
            "Episode 6/250: Total Reward: 17.0\n",
            "Episode 7/250: Total Reward: 27.0\n",
            "Episode 8/250: Total Reward: 15.0\n",
            "Episode 9/250: Total Reward: 40.0\n",
            "Episode 10/250: Total Reward: 16.0\n",
            "Episode 11/250: Total Reward: 20.0\n",
            "Episode 12/250: Total Reward: 38.0\n",
            "Episode 13/250: Total Reward: 46.0\n",
            "Episode 14/250: Total Reward: 14.0\n",
            "Episode 15/250: Total Reward: 30.0\n",
            "Episode 16/250: Total Reward: 47.0\n",
            "Episode 17/250: Total Reward: 17.0\n",
            "Episode 18/250: Total Reward: 34.0\n",
            "Episode 19/250: Total Reward: 11.0\n",
            "Episode 20/250: Total Reward: 31.0\n",
            "Episode 21/250: Total Reward: 11.0\n",
            "Episode 22/250: Total Reward: 19.0\n",
            "Episode 23/250: Total Reward: 9.0\n",
            "Episode 24/250: Total Reward: 16.0\n",
            "Episode 25/250: Total Reward: 14.0\n",
            "Episode 26/250: Total Reward: 27.0\n",
            "Episode 27/250: Total Reward: 27.0\n",
            "Episode 28/250: Total Reward: 38.0\n",
            "Episode 29/250: Total Reward: 12.0\n",
            "Episode 30/250: Total Reward: 16.0\n",
            "Episode 31/250: Total Reward: 13.0\n",
            "Episode 32/250: Total Reward: 21.0\n",
            "Episode 33/250: Total Reward: 16.0\n",
            "Episode 34/250: Total Reward: 20.0\n",
            "Episode 35/250: Total Reward: 22.0\n",
            "Episode 36/250: Total Reward: 24.0\n",
            "Episode 37/250: Total Reward: 17.0\n",
            "Episode 38/250: Total Reward: 9.0\n",
            "Episode 39/250: Total Reward: 12.0\n",
            "Episode 40/250: Total Reward: 34.0\n",
            "Episode 41/250: Total Reward: 23.0\n",
            "Episode 42/250: Total Reward: 10.0\n",
            "Episode 43/250: Total Reward: 20.0\n",
            "Episode 44/250: Total Reward: 11.0\n",
            "Episode 45/250: Total Reward: 20.0\n",
            "Episode 46/250: Total Reward: 19.0\n",
            "Episode 47/250: Total Reward: 13.0\n",
            "Episode 48/250: Total Reward: 31.0\n",
            "Episode 49/250: Total Reward: 59.0\n",
            "Episode 50/250: Total Reward: 25.0\n",
            "Episode 51/250: Total Reward: 18.0\n",
            "Episode 52/250: Total Reward: 16.0\n",
            "Episode 53/250: Total Reward: 56.0\n",
            "Episode 54/250: Total Reward: 59.0\n",
            "Episode 55/250: Total Reward: 22.0\n",
            "Episode 56/250: Total Reward: 20.0\n",
            "Episode 57/250: Total Reward: 44.0\n",
            "Episode 58/250: Total Reward: 17.0\n",
            "Episode 59/250: Total Reward: 79.0\n",
            "Episode 60/250: Total Reward: 40.0\n",
            "Episode 61/250: Total Reward: 58.0\n",
            "Episode 62/250: Total Reward: 63.0\n",
            "Episode 63/250: Total Reward: 87.0\n",
            "Episode 64/250: Total Reward: 36.0\n",
            "Episode 65/250: Total Reward: 58.0\n",
            "Episode 66/250: Total Reward: 15.0\n",
            "Episode 67/250: Total Reward: 28.0\n",
            "Episode 68/250: Total Reward: 46.0\n",
            "Episode 69/250: Total Reward: 33.0\n",
            "Episode 70/250: Total Reward: 44.0\n",
            "Episode 71/250: Total Reward: 12.0\n",
            "Episode 72/250: Total Reward: 34.0\n",
            "Episode 73/250: Total Reward: 22.0\n",
            "Episode 74/250: Total Reward: 92.0\n",
            "Episode 75/250: Total Reward: 42.0\n",
            "Episode 76/250: Total Reward: 32.0\n",
            "Episode 77/250: Total Reward: 30.0\n",
            "Episode 78/250: Total Reward: 54.0\n",
            "Episode 79/250: Total Reward: 19.0\n",
            "Episode 80/250: Total Reward: 15.0\n",
            "Episode 81/250: Total Reward: 17.0\n",
            "Episode 82/250: Total Reward: 24.0\n",
            "Episode 83/250: Total Reward: 61.0\n",
            "Episode 84/250: Total Reward: 50.0\n",
            "Episode 85/250: Total Reward: 79.0\n",
            "Episode 86/250: Total Reward: 64.0\n",
            "Episode 87/250: Total Reward: 69.0\n",
            "Episode 88/250: Total Reward: 43.0\n",
            "Episode 89/250: Total Reward: 36.0\n",
            "Episode 90/250: Total Reward: 33.0\n",
            "Episode 91/250: Total Reward: 13.0\n",
            "Episode 92/250: Total Reward: 14.0\n",
            "Episode 93/250: Total Reward: 85.0\n",
            "Episode 94/250: Total Reward: 25.0\n",
            "Episode 95/250: Total Reward: 17.0\n",
            "Episode 96/250: Total Reward: 87.0\n",
            "Episode 97/250: Total Reward: 74.0\n",
            "Episode 98/250: Total Reward: 28.0\n",
            "Episode 99/250: Total Reward: 113.0\n",
            "Episode 100/250: Total Reward: 111.0\n",
            "Episode 101/250: Total Reward: 129.0\n",
            "Episode 102/250: Total Reward: 23.0\n",
            "Episode 103/250: Total Reward: 123.0\n",
            "Episode 104/250: Total Reward: 45.0\n",
            "Episode 105/250: Total Reward: 26.0\n",
            "Episode 106/250: Total Reward: 132.0\n",
            "Episode 107/250: Total Reward: 74.0\n",
            "Episode 108/250: Total Reward: 135.0\n",
            "Episode 109/250: Total Reward: 90.0\n",
            "Episode 110/250: Total Reward: 35.0\n",
            "Episode 111/250: Total Reward: 135.0\n",
            "Episode 112/250: Total Reward: 112.0\n",
            "Episode 113/250: Total Reward: 52.0\n",
            "Episode 114/250: Total Reward: 20.0\n",
            "Episode 115/250: Total Reward: 137.0\n",
            "Episode 116/250: Total Reward: 29.0\n",
            "Episode 117/250: Total Reward: 46.0\n",
            "Episode 118/250: Total Reward: 20.0\n",
            "Episode 119/250: Total Reward: 18.0\n",
            "Episode 120/250: Total Reward: 106.0\n",
            "Episode 121/250: Total Reward: 85.0\n",
            "Episode 122/250: Total Reward: 97.0\n",
            "Episode 123/250: Total Reward: 98.0\n",
            "Episode 124/250: Total Reward: 78.0\n",
            "Episode 125/250: Total Reward: 17.0\n",
            "Episode 126/250: Total Reward: 104.0\n",
            "Episode 127/250: Total Reward: 101.0\n",
            "Episode 128/250: Total Reward: 45.0\n",
            "Episode 129/250: Total Reward: 77.0\n",
            "Episode 130/250: Total Reward: 24.0\n",
            "Episode 131/250: Total Reward: 48.0\n",
            "Episode 132/250: Total Reward: 56.0\n",
            "Episode 133/250: Total Reward: 97.0\n",
            "Episode 134/250: Total Reward: 113.0\n",
            "Episode 135/250: Total Reward: 98.0\n",
            "Episode 136/250: Total Reward: 118.0\n",
            "Episode 137/250: Total Reward: 86.0\n",
            "Episode 138/250: Total Reward: 67.0\n",
            "Episode 139/250: Total Reward: 74.0\n",
            "Episode 140/250: Total Reward: 53.0\n",
            "Episode 141/250: Total Reward: 145.0\n",
            "Episode 142/250: Total Reward: 48.0\n",
            "Episode 143/250: Total Reward: 83.0\n",
            "Episode 144/250: Total Reward: 54.0\n",
            "Episode 145/250: Total Reward: 26.0\n",
            "Episode 146/250: Total Reward: 17.0\n",
            "Episode 147/250: Total Reward: 147.0\n",
            "Episode 148/250: Total Reward: 119.0\n",
            "Episode 149/250: Total Reward: 81.0\n",
            "Episode 150/250: Total Reward: 114.0\n",
            "Episode 151/250: Total Reward: 119.0\n",
            "Episode 152/250: Total Reward: 90.0\n",
            "Episode 153/250: Total Reward: 126.0\n",
            "Episode 154/250: Total Reward: 140.0\n",
            "Episode 155/250: Total Reward: 97.0\n",
            "Episode 156/250: Total Reward: 122.0\n",
            "Episode 157/250: Total Reward: 121.0\n",
            "Episode 158/250: Total Reward: 112.0\n",
            "Episode 159/250: Total Reward: 93.0\n",
            "Episode 160/250: Total Reward: 108.0\n",
            "Episode 161/250: Total Reward: 141.0\n",
            "Episode 162/250: Total Reward: 104.0\n",
            "Episode 163/250: Total Reward: 78.0\n",
            "Episode 164/250: Total Reward: 104.0\n",
            "Episode 165/250: Total Reward: 30.0\n",
            "Episode 166/250: Total Reward: 108.0\n",
            "Episode 167/250: Total Reward: 84.0\n",
            "Episode 168/250: Total Reward: 22.0\n",
            "Episode 169/250: Total Reward: 33.0\n",
            "Episode 170/250: Total Reward: 174.0\n",
            "Episode 171/250: Total Reward: 154.0\n",
            "Episode 172/250: Total Reward: 188.0\n",
            "Episode 173/250: Total Reward: 120.0\n",
            "Episode 174/250: Total Reward: 164.0\n",
            "Episode 175/250: Total Reward: 121.0\n",
            "Episode 176/250: Total Reward: 117.0\n",
            "Episode 177/250: Total Reward: 131.0\n",
            "Episode 178/250: Total Reward: 175.0\n",
            "Episode 179/250: Total Reward: 120.0\n",
            "Episode 180/250: Total Reward: 124.0\n",
            "Episode 181/250: Total Reward: 148.0\n",
            "Episode 182/250: Total Reward: 21.0\n",
            "Episode 183/250: Total Reward: 123.0\n",
            "Episode 184/250: Total Reward: 123.0\n",
            "Episode 185/250: Total Reward: 200.0\n",
            "Evaluation Total Reward: 323.0\n"
          ]
        }
      ],
      "source": [
        "# Copyright [2025] [KTH Royal Institute of Technology]\n",
        "# Licensed under the Educational Community License, Version 2.0 (ECL-2.0)\n",
        "# This file is part of the materials for EL2805 - Reinforcement Learning - Exercise Session 3 at KTH, Stockholm.\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import deque, namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\"\"\"Tensorboard instructions:\n",
        "    1. Install library using conda install tensorboard\n",
        "    2. After running the script, open the command line (in anaconda)\n",
        "    3. Run tensorboard --logdir=runs/cartpole_dqn/\n",
        "    4. Copy the address that you get - probably http://localhost:6006/\n",
        "    5. You should be able to see logged losses, rewards and epsilon values.\n",
        "    6. If you want to log in additional quantities use writer.add_scalar() as below\"\"\"\n",
        "\n",
        "writer = SummaryWriter(log_dir=\"runs/cartpole_dqn\")\n",
        "\n",
        "\n",
        "# Define Experience tuple\n",
        "# Experience represents a transition in the environment, including the current state, action taken,\n",
        "# received reward, next state, and whether the episode is done.\n",
        "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    \"\"\"Replay buffer for storing experiences.\n",
        "\n",
        "       The experience replay buffer stores past experiences so that the agent can learn from them later.\n",
        "       By sampling randomly from these experiences, the agent avoids overfitting to the most recent\n",
        "       transitions and helps stabilize training.\n",
        "       - The buffer size is limited, and older experiences are discarded to make room for new ones.\n",
        "       - Experiences are stored as tuples of (state, action, reward, next_state, done).\n",
        "       - A batch of experiences is sampled randomly during each training step for updating the Q-values.\"\"\"\n",
        "\n",
        "    def __init__(self, maximum_length):\n",
        "        self.buffer = deque(maxlen=maximum_length)  # Using deque ensures efficient removal of oldest elements\n",
        "\n",
        "    def append(self, experience):\n",
        "        \"\"\"Add a new experience to the buffer\"\"\"\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of the buffer\"\"\"\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample_batch(self, n):\n",
        "        \"\"\"Randomly sample a batch of experiences\"\"\"\n",
        "        if n > len(self.buffer):\n",
        "            raise IndexError('Sample size exceeds buffer size!')\n",
        "        indices = np.random.choice(len(self.buffer), size=n, replace=False)  # Random sampling\n",
        "        batch = [self.buffer[i] for i in indices]  # Create a batch from sampled indices\n",
        "        return zip(*batch)  # Unzip batch into state, action, reward, next_state, and done\n",
        "\n",
        "\n",
        "class MyNetwork(nn.Module):\n",
        "    \"\"\"Feedforward neural network that approximates the Q-function.\n",
        "\n",
        "       The network takes the current state as input and outputs Q-values for all possible actions.\n",
        "       The action corresponding to the highest Q-value is considered the optimal action.\n",
        "       - The input size corresponds to the state dimension of the environment.\n",
        "       - The network has one hidden layer with 64 neurons and ReLU activation.\n",
        "       - The output layer has one neuron per action (Q-values for each action).\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.input_layer = nn.Linear(input_size, 64)  # First layer: state -> hidden layer\n",
        "        self.hidden_layer = nn.Linear(64, 64)  # Second layer: hidden -> hidden layer\n",
        "        self.output_layer = nn.Linear(64, output_size)  # Output layer: hidden -> Q-values\n",
        "        self.activation = nn.ReLU()  # ReLU activation function for hidden layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Define forward pass\"\"\"\n",
        "        x = self.activation(self.input_layer(x))  # Apply input layer and ReLU\n",
        "        x = self.activation(self.hidden_layer(x))  # Apply hidden layer and ReLU\n",
        "        return self.output_layer(x)  # Return Q-values for all actions\n",
        "\n",
        "\n",
        "### Parameters ###\n",
        "GAMMA = 0.99  # Discount factor (how much future rewards are considered)\n",
        "EPSILON = 1.0  # Initial exploration rate (balance between exploration and exploitation)\n",
        "EPSILON_MIN = 0.01  # Minimum exploration rate\n",
        "EPSILON_DECAY = 0.995  # Decay rate for epsilon after each episode\n",
        "BATCH_SIZE = 32  # Number of experiences to sample from the replay buffer per update\n",
        "BUFFER_SIZE = 10000  # Size of the replay buffer\n",
        "LEARNING_RATE = 0.001  # Learning rate for the optimizer\n",
        "N_EPISODES = 250  # Number of training episodes\n",
        "MAX_STEPS = 200  # Maximum number of steps per episode\n",
        "\n",
        "# Initialize environment, buffer, network, and optimizer\n",
        "env = gym.make('CartPole-v1')  # Create the CartPole environment\n",
        "\n",
        "# Initialize experience replay buffer\n",
        "buffer = ExperienceReplayBuffer(maximum_length=BUFFER_SIZE)\n",
        "\n",
        "# Initialize the Q-network (state -> Q-values for actions)\n",
        "network = MyNetwork(input_size=env.observation_space.shape[0], output_size=env.action_space.n)\n",
        "\n",
        "# Optimizer for training the Q-network\n",
        "optimizer = optim.Adam(network.parameters(), lr=LEARNING_RATE)  # Adam optimizer for efficient training\n",
        "\n",
        "\n",
        "\n",
        "def select_action(state, epsilon):\n",
        "    \"\"\"Epsilon-greedy action selection\n",
        "    # We balance exploration and exploitation using epsilon-greedy.\n",
        "    # Exploration: Choose a random action.\n",
        "    # Exploitation: Choose the action with the highest Q-value (the optimal action).\"\"\"\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()  # Explore by selecting a random action\n",
        "    else:\n",
        "        state_tensor = torch.tensor([state], dtype=torch.float32)  # Convert state to tensor\n",
        "        return network(state_tensor).argmax().item()  # Exploit by selecting the action with max Q-value\n",
        "\n",
        "# Training loop\n",
        "for episode in range(N_EPISODES):\n",
        "    state = env.reset()[0]  # Reset environment and get initial state\n",
        "    total_reward = 0\n",
        "    for t in range(MAX_STEPS):\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        action = select_action(state, EPSILON)\n",
        "\n",
        "        # Execute action in environment and get feedback (next state, reward, etc.)\n",
        "        next_state, reward, terminal, truncated, _ = env.step(action)\n",
        "        done = terminal or truncated  # Done is True if episode ends\n",
        "        total_reward += reward\n",
        "\n",
        "        # Store the experience (state, action, reward, next state, done) in the buffer\n",
        "        buffer.append(Experience(state, action, reward, next_state, done))\n",
        "        state = next_state  # Update state for the next step\n",
        "\n",
        "        # Training step: update Q-values using a batch of experiences from the buffer\n",
        "        if len(buffer) >= BATCH_SIZE:\n",
        "            # Sample a batch of experiences from the buffer\n",
        "            states, actions, rewards, next_states, dones = buffer.sample_batch(BATCH_SIZE)\n",
        "\n",
        "            # Convert the batch data into tensors\n",
        "            states = torch.tensor(states, dtype=torch.float32)\n",
        "            actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)  # Unsqueeze for correct shape\n",
        "            rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "            next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "            dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "            # Compute Q-values for the current states\n",
        "            q_values = network(states).gather(1, actions).squeeze()  # Q-values for taken actions\n",
        "\n",
        "            # Compute the target Q-values for the next states\n",
        "            with torch.no_grad():  # No need to compute gradients for target Q-values\n",
        "                next_q_values = network(next_states).max(1)[0]  # Max Q-value for next state\n",
        "                targets = rewards + GAMMA * next_q_values * (1 - dones)  # Target: Bellman equation\n",
        "\n",
        "            # Compute the loss (MSE loss between predicted Q-values and target Q-values)\n",
        "            loss = nn.functional.mse_loss(q_values, targets)\n",
        "\n",
        "            # Backpropagation step: update network parameters\n",
        "            optimizer.zero_grad()  # Zero gradients before backpropagation\n",
        "            loss.backward()  # Compute gradients\n",
        "            nn.utils.clip_grad_norm_(network.parameters(), max_norm=1.0)  # Clip gradients to avoid exploding gradients\n",
        "            optimizer.step()  # Update parameters\n",
        "\n",
        "            # Log loss to TensorBoard for visualization\n",
        "            writer.add_scalar(\"Loss\", loss.item(), episode * MAX_STEPS + t)\n",
        "\n",
        "        if done:  # If the episode ends\n",
        "            break\n",
        "\n",
        "    # Decay epsilon: reduce exploration over time\n",
        "    EPSILON = max(EPSILON_MIN, EPSILON * EPSILON_DECAY)\n",
        "\n",
        "    # Log total reward and epsilon to TensorBoard\n",
        "    writer.add_scalar(\"Total Reward\", total_reward, episode)\n",
        "    writer.add_scalar(\"Epsilon\", EPSILON, episode)\n",
        "\n",
        "    # Print progress for each episode\n",
        "    print(f\"Episode {episode + 1}/{N_EPISODES}: Total Reward: {total_reward}\")\n",
        "    if total_reward >= 200:  # If the agent achieves good performance, stop early\n",
        "        break\n",
        "\n",
        "# Close the environment after training\n",
        "env.close()\n",
        "\n",
        "\n",
        "# Evaluate the trained policy by rendering it\n",
        "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
        "state = env.reset()[0]\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "# Run the trained agent in the environment\n",
        "while not done:\n",
        "    env.render()\n",
        "    action = select_action(state, 0)  # Choose action (epsilon=0, i.e., exploit the policy)\n",
        "    next_state, reward, done, truncated, _ = env.step(action)\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "\n",
        "print(f\"Evaluation Total Reward: {total_reward}\")\n",
        "env.close()\n",
        "\n",
        "writer.close()\n",
        "\n"
      ],
      "id": "HISBWdKAJDME"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
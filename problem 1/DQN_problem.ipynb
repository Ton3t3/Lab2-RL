{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ton3t3/Lab2-RL/blob/main/problem%201/DQN_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Ton3t3/Lab2-RL\n",
        "\n",
        "%cd Lab2-RL/problem 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iHCJFrvhQM-h",
        "outputId": "5211fa48-a2e8-41ec-bab7-e3b2b771d341"
      },
      "id": "iHCJFrvhQM-h",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Lab2-RL'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 28 (delta 13), reused 6 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (28/28), 455.06 KiB | 2.86 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "/content/Lab2-RL/problem 1/Lab2-RL/problem 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "abce98e9",
      "metadata": {
        "id": "abce98e9"
      },
      "outputs": [],
      "source": [
        "# Copyright [2025] [KTH Royal Institute of Technology]\n",
        "# Licensed under the Educational Community License, Version 2.0 (ECL-2.0)\n",
        "# This file is part of the Computer Lab 2 for EL2805 - Reinforcement Learning.\n",
        "\n",
        "# Load packages\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange\n",
        "from DQN_agent import RandomAgent\n",
        "import warnings\n",
        "import random\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\""
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRmLRKtJT4zB",
        "outputId": "78fbfc0f-42d7-4545-9f59-e5f59481e134"
      },
      "id": "LRmLRKtJT4zB",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3bfa8372",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "3bfa8372",
        "outputId": "51ef070b-c27a-4d3b-9364-9ca8a1377324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpisode:   0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipython-input-3528538243.py:152: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  states = torch.tensor(states, dtype=torch.float32)\n",
            "Episode 10 - Reward/Steps: -92.1/66 - Avg. Reward/Steps: 0.0/0:  11%|█         | 11/100 [00:05<00:44,  2.01it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3528538243.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Zero gradients before backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m           \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clip gradients to avoid exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0m_clip_grads_with_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_clip_grads_with_norm_\u001b[0;34m(parameters, max_norm, total_norm, foreach)\u001b[0m\n\u001b[1;32m    163\u001b[0m     ] = _group_tensors_by_device_and_dtype([grads])  # type: ignore[assignment]\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;31m# Note: multiplying by the clamped coef is redundant when the coef is clamped to 1, but doing so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# avoids a `if clip_coef < 1:` conditional which can require a CPU <=> device synchronization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_TensorLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tensor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m ) -> Callable[Concatenate[_TensorLike, _P], \"Tensor\"]:\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_TensorLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Tensor\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "## EXPERIENCE REPLAY BUFFER IMPLEMENTATION\n",
        "## (Copy from DQNelements_solved-1.py)\n",
        "\n",
        "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    \"\"\"Replay buffer for storing experiences.\n",
        "\n",
        "       The experience replay buffer stores past experiences so that the agent can learn from them later.\n",
        "       By sampling randomly from these experiences, the agent avoids overfitting to the most recent\n",
        "       transitions and helps stabilize training.\n",
        "       - The buffer size is limited, and older experiences are discarded to make room for new ones.\n",
        "       - Experiences are stored as tuples of (state, action, reward, next_state, done).\n",
        "       - A batch of experiences is sampled randomly during each training step for updating the Q-values.\"\"\"\n",
        "\n",
        "    def __init__(self, maximum_length):\n",
        "        self.buffer = deque(maxlen=maximum_length)  # Using deque ensures efficient removal of oldest elements\n",
        "\n",
        "    def append(self, experience):\n",
        "        \"\"\"Add a new experience to the buffer\"\"\"\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of the buffer\"\"\"\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample_batch(self, n):\n",
        "        \"\"\"Randomly sample a batch of experiences\"\"\"\n",
        "        if n > len(self.buffer):\n",
        "            raise IndexError('Sample size exceeds buffer size!')\n",
        "        indices = np.random.choice(len(self.buffer), size=n, replace=False)  # Random sampling\n",
        "        batch = [self.buffer[i] for i in indices]  # Create a batch from sampled indices\n",
        "        return zip(*batch)  # Unzip batch into state, action, reward, next_state, and done\n",
        "##\n",
        "## NEURAL NETWORK IMPLEMENTATION\n",
        "## (Copy from DQNelements_solved-1.py)\n",
        "\n",
        "'''\n",
        "INTERESTING PARAMETERS OF THE NEURAL NETWORK:\n",
        "    · Number of layers -> 1\n",
        "    · Number of neurons per layer -> 64\n",
        "    · Number of hidden layers -> 1\n",
        "    · Activation function -> ReLU\n",
        "'''\n",
        "class MyNetwork(nn.Module):\n",
        "    \"\"\"Feedforward neural network that approximates the Q-function.\n",
        "\n",
        "       The network takes the current state as input and outputs Q-values for all possible actions.\n",
        "       The action corresponding to the highest Q-value is considered the optimal action.\n",
        "       - The input size corresponds to the state dimension of the environment.\n",
        "       - The network has one hidden layer with 64 neurons and ReLU activation.\n",
        "       - The output layer has one neuron per action (Q-values for each action).\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, neurons_per_layer):\n",
        "        super().__init__()\n",
        "        self.input_layer = nn.Linear(input_size, neurons_per_layer)  # First layer: state -> hidden layer\n",
        "        self.hidden_layer = nn.Linear(neurons_per_layer, neurons_per_layer)  # Second layer: hidden -> hidden layer\n",
        "        self.output_layer = nn.Linear(neurons_per_layer, output_size)  # Output layer: hidden -> Q-values\n",
        "        self.activation = nn.ReLU()  # ReLU activation function for hidden layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Define forward pass\"\"\"\n",
        "        x = self.activation(self.input_layer(x))  # Apply input layer and ReLU\n",
        "        x = self.activation(self.hidden_layer(x))  # Apply hidden layer and ReLU\n",
        "        return self.output_layer(x)  # Return Q-values for all actions\n",
        "##\n",
        "\n",
        "def running_average(x, N):\n",
        "    ''' Function used to compute the running average\n",
        "        of the last N elements of a vector x\n",
        "    '''\n",
        "    if len(x) >= N:\n",
        "        y = np.copy(x)\n",
        "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
        "    else:\n",
        "        y = np.zeros_like(x)\n",
        "    return y\n",
        "\n",
        "# Import and initialize the discrete Lunar Lander Environment\n",
        "env = gym.make('LunarLander-v3')\n",
        "# If you want to render the environment while training run instead:\n",
        "# env = gym.make('LunarLander-v3', render_mode = \"human\")\n",
        "\n",
        "env.reset()\n",
        "\n",
        "#Extra parameters (not included in raw version of the exercise)\n",
        "BUFFER_SIZE = 10000                                 # Size of the replay buffer (5000 - 30000)\n",
        "TRAINING_BATCH = 32                                 # How big should be the batch of experiences used to train the main NN (4 - 128)\n",
        "C = int(BUFFER_SIZE/TRAINING_BATCH)                 # Update frequency of the target\n",
        "LEARNING_RATE = 0.001                               # (10-3 - 10-4)\n",
        "NEURONS_PER_LAYER = 64                              # Number of neurons per hidden layer (8 - 128)\n",
        "# Parameters\n",
        "N_episodes = 100                             # Number of episodes\n",
        "discount_factor = 0.95                       # Value of the discount factor\n",
        "n_ep_running_average = 50                    # Running average of 50 episodes\n",
        "n_actions = env.action_space.n               # Number of available actions\n",
        "dim_state = len(env.observation_space.high)  # State dimensionality\n",
        "\n",
        "# Initialize buffer, network, and optimizer\n",
        "buffer = ExperienceReplayBuffer(BUFFER_SIZE)\n",
        "network = MyNetwork(input_size=env.observation_space.shape[0], output_size=env.action_space.n, neurons_per_layer=NEURONS_PER_LAYER)\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "def select_action(state, epsilon):\n",
        "    \"\"\"Epsilon-greedy action selection\"\"\"\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()  # Explore by selecting a random action\n",
        "    else: # 1-epsilon\n",
        "        state_tensor = torch.tensor([state], dtype=torch.float32)  # Convert state to tensor\n",
        "        return network(state_tensor).argmax().item()  # Exploit by selecting the action with max Q-value\n",
        "\n",
        "\n",
        "# We will use these variables to compute the average episodic reward and\n",
        "# the average number of steps per episode\n",
        "episode_reward_list = []       # this list contains the total reward per episode\n",
        "episode_number_of_steps = []   # this list contains the number of steps per episode\n",
        "\n",
        "# Random agent initialization\n",
        "agent = RandomAgent(n_actions)\n",
        "\n",
        "### Training process\n",
        "\n",
        "# trange is an alternative to range in python, from the tqdm library\n",
        "# It shows a nice progression bar that you can update with useful information\n",
        "EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
        "\n",
        "for i in EPISODES:\n",
        "    # Reset enviroment data and initialize variables\n",
        "    done, truncated = False, False\n",
        "    state = env.reset()[0]\n",
        "    total_episode_reward = 0.\n",
        "    t = 0\n",
        "    while not (done or truncated):\n",
        "        # Take a random action\n",
        "        action = agent.forward(state)\n",
        "\n",
        "        # Get next state and reward\n",
        "        next_state, reward, done, truncated, _ = env.step(action)\n",
        "\n",
        "        # Update episode reward\n",
        "        total_episode_reward += reward\n",
        "\n",
        "        # Update state for next iteration\n",
        "        ## Add new state to the buffer\n",
        "        buffer.append(Experience(state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "        t+= 1\n",
        "\n",
        "        if len(buffer) >= TRAINING_BATCH:\n",
        "          states, actions, rewards, next_states, dones = buffer.sample_batch(TRAINING_BATCH)\n",
        "          # Convert the batch data into tensors\n",
        "          states = torch.tensor(states, dtype=torch.float32)\n",
        "          actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)  # Unsqueeze for correct shape\n",
        "          rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "          next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "          dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "          # Compute Q-values for the current states\n",
        "          q_values = network(states).gather(1, actions).squeeze()  # Q-values for taken actions\n",
        "\n",
        "          # Compute the target Q-values for the next states\n",
        "          with torch.no_grad():  # No need to compute gradients for target Q-values\n",
        "              next_q_values = network(next_states).max(1)[0]  # Max Q-value for next state\n",
        "              targets = rewards + discount_factor * next_q_values * (1 - dones)  # Target: Bellman equation\n",
        "\n",
        "          # Compute the loss (MSE loss between predicted Q-values and target Q-values)\n",
        "          loss = nn.functional.mse_loss(q_values, targets)\n",
        "\n",
        "          # Backpropagation step: update network parameters\n",
        "          optimizer.zero_grad()  # Zero gradients before backpropagation\n",
        "          loss.backward()  # Compute gradients\n",
        "          nn.utils.clip_grad_norm_(network.parameters(), max_norm=1.0)  # Clip gradients to avoid exploding gradients\n",
        "          optimizer.step()  # Update parameters\n",
        "\n",
        "          # # Log loss to TensorBoard for visualization\n",
        "          # writer.add_scalar(\"Loss\", loss.item(), episode * MAX_STEPS + t)\n",
        "\n",
        "    # Append episode reward and total number of steps\n",
        "    episode_reward_list.append(total_episode_reward)\n",
        "    episode_number_of_steps.append(t)\n",
        "\n",
        "\n",
        "    # Updates the tqdm update bar with fresh information\n",
        "    # (episode number, total reward of the last episode, total number of Steps\n",
        "    # of the last episode, average reward, average number of steps)\n",
        "    EPISODES.set_description(\n",
        "        \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{}\".format(\n",
        "        i, total_episode_reward, t,\n",
        "        running_average(episode_reward_list, n_ep_running_average)[-1],\n",
        "        running_average(episode_number_of_steps, n_ep_running_average)[-1]))\n",
        "\n",
        "# Close environment\n",
        "env.close()\n",
        "\n",
        "# Plot Rewards and steps\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
        "ax[0].plot([i for i in range(1, N_episodes+1)], episode_reward_list, label='Episode reward')\n",
        "ax[0].plot([i for i in range(1, N_episodes+1)], running_average(\n",
        "    episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
        "ax[0].set_xlabel('Episodes')\n",
        "ax[0].set_ylabel('Total reward')\n",
        "ax[0].set_title('Total Reward vs Episodes')\n",
        "ax[0].legend()\n",
        "ax[0].grid(alpha=0.3)\n",
        "\n",
        "ax[1].plot([i for i in range(1, N_episodes+1)], episode_number_of_steps, label='Steps per episode')\n",
        "ax[1].plot([i for i in range(1, N_episodes+1)], running_average(\n",
        "    episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
        "ax[1].set_xlabel('Episodes')\n",
        "ax[1].set_ylabel('Total number of steps')\n",
        "ax[1].set_title('Total number of steps vs Episodes')\n",
        "ax[1].legend()\n",
        "ax[1].grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
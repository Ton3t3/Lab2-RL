{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright [2025] [KTH Royal Institute of Technology] \n",
    "# Licensed under the Educational Community License, Version 2.0 (ECL-2.0)\n",
    "# This file is part of the Computer Lab 2 for EL2805 - Reinforcement Learning.\n",
    "\n",
    "# Load packages\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from DQN_agent import RandomAgent\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa8372",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## EXPERIENCE REPLAY BUFFER IMPLEMENTATION \n",
    "## (Copy from DQNelements_solved-1.py)\n",
    "class ExperienceReplayBuffer:\n",
    "    \"\"\"Replay buffer for storing experiences.\n",
    "    \n",
    "       The experience replay buffer stores past experiences so that the agent can learn from them later.\n",
    "       By sampling randomly from these experiences, the agent avoids overfitting to the most recent \n",
    "       transitions and helps stabilize training.\n",
    "       - The buffer size is limited, and older experiences are discarded to make room for new ones.\n",
    "       - Experiences are stored as tuples of (state, action, reward, next_state, done).\n",
    "       - A batch of experiences is sampled randomly during each training step for updating the Q-values.\"\"\"\n",
    "\n",
    "    def __init__(self, maximum_length):\n",
    "        self.buffer = deque(maxlen=maximum_length)  # Using deque ensures efficient removal of oldest elements\n",
    "\n",
    "    def append(self, experience):\n",
    "        \"\"\"Add a new experience to the buffer\"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of the buffer\"\"\"\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample_batch(self, n):\n",
    "        \"\"\"Randomly sample a batch of experiences\"\"\"\n",
    "        if n > len(self.buffer):\n",
    "            raise IndexError('Sample size exceeds buffer size!')\n",
    "        indices = np.random.choice(len(self.buffer), size=n, replace=False)  # Random sampling\n",
    "        batch = [self.buffer[i] for i in indices]  # Create a batch from sampled indices\n",
    "        return zip(*batch)  # Unzip batch into state, action, reward, next_state, and done\n",
    "##\n",
    "## NEURAL NETWORK IMPLEMENTATION\n",
    "## (Copy from DQNelements_solved-1.py)\n",
    "\n",
    "'''\n",
    "INTERESTING PARAMETERS OF THE NEURAL NETWORK: \n",
    "    路 Number of layers -> 1\n",
    "    路 Number of neurons per layer -> 64\n",
    "    路 Number of hidden layers -> 1\n",
    "    路 Activation function -> ReLU\n",
    "'''\n",
    "class MyNetwork(nn.Module):\n",
    "    \"\"\"Feedforward neural network that approximates the Q-function.\n",
    "    \n",
    "       The network takes the current state as input and outputs Q-values for all possible actions.\n",
    "       The action corresponding to the highest Q-value is considered the optimal action.\n",
    "       - The input size corresponds to the state dimension of the environment.\n",
    "       - The network has one hidden layer with 64 neurons and ReLU activation.\n",
    "       - The output layer has one neuron per action (Q-values for each action).\"\"\"\n",
    "        \n",
    "    def __init__(self, input_size, output_size, neurons_per_layer):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_size, neurons_per_layer)  # First layer: state -> hidden layer\n",
    "        self.hidden_layer = nn.Linear(neurons_per_layer, neurons_per_layer)  # Second layer: hidden -> hidden layer\n",
    "        self.output_layer = nn.Linear(neurons_per_layer, output_size)  # Output layer: hidden -> Q-values\n",
    "        self.activation = nn.ReLU()  # ReLU activation function for hidden layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Define forward pass\"\"\"\n",
    "        x = self.activation(self.input_layer(x))  # Apply input layer and ReLU\n",
    "        x = self.activation(self.hidden_layer(x))  # Apply hidden layer and ReLU\n",
    "        return self.output_layer(x)  # Return Q-values for all actions\n",
    "##\n",
    "\n",
    "def running_average(x, N):\n",
    "    ''' Function used to compute the running average\n",
    "        of the last N elements of a vector x\n",
    "    '''\n",
    "    if len(x) >= N:\n",
    "        y = np.copy(x)\n",
    "        y[N-1:] = np.convolve(x, np.ones((N, )) / N, mode='valid')\n",
    "    else:\n",
    "        y = np.zeros_like(x)\n",
    "    return y\n",
    "\n",
    "# Import and initialize the discrete Lunar Lander Environment\n",
    "env = gym.make('LunarLander-v3')\n",
    "# If you want to render the environment while training run instead:\n",
    "# env = gym.make('LunarLander-v3', render_mode = \"human\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "#Extra parameters (not included in raw version of the exercise)\n",
    "BUFFER_SIZE = 10000                                 # Size of the replay buffer (5000 - 30000)\n",
    "TRAINING_BATCH = 32                                 # How big should be the batch of experiences used to train the main NN (4 - 128)\n",
    "C = int(BUFFER_SIZE/TRAINING_BATCH)                 # Update frequency of the target\n",
    "LEARNING_RATE = 0.001                               # (10-3 - 10-4)\n",
    "NEURONS_PER_LAYER = 64                              # Number of neurons per hidden layer (8 - 128)\n",
    "# Parameters\n",
    "N_episodes = 100                             # Number of episodes\n",
    "discount_factor = 0.95                       # Value of the discount factor\n",
    "n_ep_running_average = 50                    # Running average of 50 episodes\n",
    "n_actions = env.action_space.n               # Number of available actions\n",
    "dim_state = len(env.observation_space.high)  # State dimensionality\n",
    "\n",
    "# Initialize buffer, network, and optimizer\n",
    "buffer = ExperienceReplayBuffer(BUFFER_SIZE)\n",
    "network = MyNetwork(input_size=env.observation_space.shape[0], output_size=env.action_space.n, neurons_per_layer=NEURONS_PER_LAYER)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# We will use these variables to compute the average episodic reward and\n",
    "# the average number of steps per episode\n",
    "episode_reward_list = []       # this list contains the total reward per episode\n",
    "episode_number_of_steps = []   # this list contains the number of steps per episode\n",
    "\n",
    "# Random agent initialization\n",
    "agent = RandomAgent(n_actions)\n",
    "\n",
    "### Training process\n",
    "\n",
    "# trange is an alternative to range in python, from the tqdm library\n",
    "# It shows a nice progression bar that you can update with useful information\n",
    "EPISODES = trange(N_episodes, desc='Episode: ', leave=True)\n",
    "\n",
    "for i in EPISODES:\n",
    "    # Reset enviroment data and initialize variables\n",
    "    done, truncated = False, False\n",
    "    state = env.reset()[0]\n",
    "    total_episode_reward = 0.\n",
    "    t = 0\n",
    "    while not (done or truncated):\n",
    "        # Take a random action\n",
    "        action = agent.forward(state)\n",
    "\n",
    "        # Get next state and reward\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        # Update episode reward\n",
    "        total_episode_reward += reward\n",
    "\n",
    "        # Update state for next iteration\n",
    "        state = next_state\n",
    "        t+= 1\n",
    "\n",
    "    # Append episode reward and total number of steps\n",
    "    episode_reward_list.append(total_episode_reward)\n",
    "    episode_number_of_steps.append(t)\n",
    "\n",
    "\n",
    "    # Updates the tqdm update bar with fresh information\n",
    "    # (episode number, total reward of the last episode, total number of Steps\n",
    "    # of the last episode, average reward, average number of steps)\n",
    "    EPISODES.set_description(\n",
    "        \"Episode {} - Reward/Steps: {:.1f}/{} - Avg. Reward/Steps: {:.1f}/{}\".format(\n",
    "        i, total_episode_reward, t,\n",
    "        running_average(episode_reward_list, n_ep_running_average)[-1],\n",
    "        running_average(episode_number_of_steps, n_ep_running_average)[-1]))\n",
    "\n",
    "# Close environment\n",
    "env.close()\n",
    "\n",
    "# Plot Rewards and steps\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 9))\n",
    "ax[0].plot([i for i in range(1, N_episodes+1)], episode_reward_list, label='Episode reward')\n",
    "ax[0].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "    episode_reward_list, n_ep_running_average), label='Avg. episode reward')\n",
    "ax[0].set_xlabel('Episodes')\n",
    "ax[0].set_ylabel('Total reward')\n",
    "ax[0].set_title('Total Reward vs Episodes')\n",
    "ax[0].legend()\n",
    "ax[0].grid(alpha=0.3)\n",
    "\n",
    "ax[1].plot([i for i in range(1, N_episodes+1)], episode_number_of_steps, label='Steps per episode')\n",
    "ax[1].plot([i for i in range(1, N_episodes+1)], running_average(\n",
    "    episode_number_of_steps, n_ep_running_average), label='Avg. number of steps per episode')\n",
    "ax[1].set_xlabel('Episodes')\n",
    "ax[1].set_ylabel('Total number of steps')\n",
    "ax[1].set_title('Total number of steps vs Episodes')\n",
    "ax[1].legend()\n",
    "ax[1].grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
